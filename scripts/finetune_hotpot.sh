torchrun --nproc_per_node=4 --master_port 20640 finetune_hotpot.py \
        --output_dir output/lloco_hotpot \
        --run_name lloco-hotpot \
        --embedding_path ./embeddings/hqa_train_embs.pth \
        --num_train_epochs 3  \
        --per_device_train_batch_size 1     \
        --per_device_eval_batch_size 1     \
        --gradient_accumulation_steps 4     \
        --evaluation_strategy "no"     \
        --save_strategy "steps"     \
        --save_steps 10000     \
        --save_total_limit 2     \
        --learning_rate 2e-5     \
        --weight_decay 0.0     \
        --warmup_ratio 0.04 \
        --lora_r 8 \
        --lora_alpha 16 \
        --lr_scheduler_type "cosine" \
        --logging_steps 1     \
        --deepspeed "ds_configs/stage2.json" \
        --bf16 True \
        --tf32 True \
        --lazy_preprocess True \
        --emb_model_name "autocomp" \
        --report_to "wandb" \
        --eval_mode "autocomp" \
        --n_sample 20000     \